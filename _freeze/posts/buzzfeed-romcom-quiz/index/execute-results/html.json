{
  "hash": "682d274ad96cf7b29149ee1ff161e710",
  "result": {
    "markdown": "---\ntitle: \"Cheat Your Way to Rom-Com Mastery: Unleashing Chat-GPT on the Buzzfeed Quiz\"\ndescription: \"How to take advantage of Generative AI to complete pointless Buzzfeed Quizzes.\"\nauthor: \"Alex Wainwright\"\ndate: 2024-02-18\ntitle-block-banner-color: \"white\"\ncategories:\n  - Generative AI\n  - Buzzfeed\nfreeze: true\n---\n\n## Background\n\nRomcoms are not my forte so a [Buzzfeed quiz](https://www.buzzfeed.com/elizabeth_cotton/rom-com-movies-bad-descriptions-quiz) to guess film titles based on a *bad description* is a recipe for disaster. The good thing, however, is that it presents a great use case for taking advantage of Generative AI (e.g., Chat-GPT).\n\n## Aim\n\nNot only do we want to leverage Generative AI to complete the 15 item quiz, but we want to automate the answering of each question. That is, we want to identify each item, extract the *bad description*, submit this to Chat-GPT, obtain an answer, submit the answer to Buzzfeed, and eventually obtain our final score. Setting this up probably took longer than manually completing the quiz, but let's gloss over that point.\n\n## How\n\nFor this project, we use the OpenAI and Playwright packages. The OpenAI package allows us to interact with both the Chat-GPT 3.5 and 4 end-points, for which we submit out *bad descriptions* to. Playwright will automate our browser interactions to emulate a quiz taker.\n\nThe steps in the script are as follows:\n\n1.  We are using a synchronous instance of Playwright, which we direct to the quiz page.\n\n2.  On the quiz page, we need to locate the quiz section that contains the 15 items, which is achieved by `page.locator(\".question__iRCfm\").all()`. This is locating the named class element and returning the list of elements contained within (i.e., the individual quiz items).\n\n3.  For each quiz item, we extract the inner text (i.e., the *bad description*).\n\n4.  We then setup Chat-GPT. The prompt used was as follows:\n\n    > I'm going to give you a description of an actual film. I want you to give me the answer. Provide just the film title, nothing else.\n\n    We are explicitly telling Chat-GPT to just provide the movie title based on the description. Without this, Chat-GPT would provide its reasoning on how it got to this answer, which is irrelevant for our purpose.\n\n5.  We extract the Chat-GPT answer, find the answer box for the quiz item, and click guess.\n\n6.  At this point, the guess will be correct and we can move on to the next item. In the event the answer is incorrect, the quiz allows us to re-attempt our answer. For this setup, we're only allowing Chat-GPT to have one attempt at answering the question so we find the *I give up!* button and click it, leading us onto the next item.\n\n7.  After we've answered each item, the quiz will generate our score. We used the Playwright screenshot function to capture the scorecard, which tells us how many items we got correct and how we compared to others.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nimport os\nfrom playwright.sync_api import sync_playwright\n\nload_dotenv()\n\nif __name__ == \"__main__\":\n\n    client = OpenAI(\n        api_key=os.environ.get(\"OPEN_AI_KEY\"),\n    )\n\n    with sync_playwright() as p:\n        browser = p.chromium.launch(headless=True)\n        page = browser.new_page()\n        page.goto(\n            \"https://www.buzzfeed.com/elizabeth_cotton/rom-com-movies-bad-descriptions-quiz\")\n        \n        question_elements = (\n            page\n            .locator(\".question__iRCfm\")\n            .all()\n        )\n\n        for question_element in question_elements:\n            \n            question_text = (\n                question_element\n                .locator(\".questionTileTitle__NxVlZ\")\n                .inner_text()\n            )\n\n            # Submit Question to Chat-GPT -----------------\n\n            chat_gpt_messages = [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"I'm going to give you a description of an actual film. I want you to give me the answer. Provide just the film title, nothing else.\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": question_text\n                },\n            ]\n\n            chat_completion = client.chat.completions.create(\n                messages=chat_gpt_messages,\n                model=\"gpt-3.5-turbo-0125\",\n            )\n\n            chat_gpt_answer = (\n                chat_completion\n                .model_dump()[\"choices\"][0][\"message\"][\"content\"]\n            )\n\n            # Identify Question Answer Box ------\n\n            question_answer = (\n                question_element\n                .get_by_label(\"Your Answer\")\n                .fill(chat_gpt_answer)\n            )\n\n            (\n                question_element\n                .get_by_role(\"button\", name=\"Guess\")\n                .click()\n            )\n\n            give_up_button = (\n                question_element\n                .get_by_role(\"button\", name=\"I give up!\")\n            )\n\n            if give_up_button.is_visible():\n                give_up_button.click()\n\n        \n        (\n            page\n            .locator(\".gradient__R2MwP\")\n            .screenshot(path=\"output/buzzfeed_romcom_decscription_quiz/chat_gpt_3_scorecard.png\")\n        )\n```\n:::\n\n\n## Results\n\nBoth Chat-GPT 3.5 and 4 were tested. Each performed far better than me ðŸ˜…. Moreover, we see that Chat-GPT 4 outperformed 3.5 in identifying romcoms based on *bad descriptions* alone.\n\n![Chat-GPT 3.5 results on the romcom bad description movie quiz. The model performed well with a score of 10 out of 15, which is better than 68% of all other quiz-takers.](/_site/posts/buzzfeed-romcom-quiz/quiz_screenshots/chat_gpt_3_scorecard.png)\n\n![Chat-GPT 4 results on the romcom bad description movie quiz. The model performed well with a score of 12 out of 15, which is better both better than Chat-GPT 3.5 and better than 90% of all other quiz-takers.](/_site/posts/buzzfeed-romcom-quiz/quiz_screenshots/chat_gpt_4_scorecard.png)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}